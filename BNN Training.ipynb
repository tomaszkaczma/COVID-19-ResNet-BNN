{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944c0ed6",
   "metadata": {},
   "source": [
    "#BNN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ab70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchbnn as bnn\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#from torchbnn import transform_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06131522",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = torch.load('trainlist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96e9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid = torch.load('vallist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652d49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"lr\": np.random.uniform(1e-4, 1e-1),\n",
    "    \"batch_size\": np.random.choice([64]),\n",
    "   # \"momentum\": np.random.uniform( 0.1,0.5, 0.9)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7e59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = DataLoader(Train, int(config[\"batch_size\"]),\n",
    "train_dataloader = DataLoader(Train, int(config[\"batch_size\"]),\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35432f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(Valid, int(config[\"batch_size\"]),\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd436a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BCNN MODEL\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "def transform_layer(input, from_inst, to_inst, args={}, attrs={}):\n",
    "    if isinstance(input, from_inst) :\n",
    "        for key in args.keys() :\n",
    "            arg = args[key]\n",
    "            if isinstance(arg, str) :\n",
    "                if arg.startswith(\".\") :\n",
    "                    args[key] = getattr(input, arg[1:])\n",
    "                    \n",
    "        output = to_inst(**args)\n",
    "        \n",
    "        for key in attrs.keys() :\n",
    "            attr = attrs[key]\n",
    "            if isinstance(attr, str) :\n",
    "                if attr.startswith(\".\") :\n",
    "                    attrs[key] = getattr(input, attr[1:])\n",
    "        \n",
    "            setattr(output, key, attrs[key])\n",
    "    else :\n",
    "        output = input        \n",
    "    return output\n",
    "\n",
    "\n",
    "def transform_model(input, from_inst, to_inst, args={}, attrs={}, inplace=True, _warn=True):\n",
    "    if inplace :\n",
    "        output = input\n",
    "        if _warn :\n",
    "            warnings.warn(\"\\n * Caution : The Input Model is CHANGED because inplace=True.\", Warning)\n",
    "    else :\n",
    "        output = copy.deepcopy(input)\n",
    "    \n",
    "    if isinstance(output, from_inst) :\n",
    "        output = transform_layer(output, from_inst, to_inst, copy.deepcopy(args), copy.deepcopy(attrs))\n",
    "    else :\n",
    "        for name, module in output.named_children() :\n",
    "            setattr(output, name, transform_model(module, from_inst, to_inst, copy.deepcopy(args), copy.deepcopy(attrs), _warn=False))\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533dfd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=3 ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1,1)))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "       # self.fc =  bnn.BayesLinear(prior_mu=0, prior_sigma=0.01, in_features=128, out_features=3),\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ea419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet2(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The output of your last Conv2d would be like (N, 64, 7, 7), \n",
    "#where N stands for batch_size, 64 for number of channels and 7x7, the height and width of the image.\n",
    "#So, now Flatten() will convert this into shape (N, 64 x 7 x 7). \n",
    "#Now, when it will go to the first Linear, the output will be (N, 100) and after second Linear (N, 10).\n",
    "\n",
    "\n",
    "\n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        \n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128)\n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv_layer(x)\n",
    "        out = out.view(x.size(0), -1)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out\n",
    "ConvNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e9bc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ConvNet2()\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec326cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0cf4f14a0383>:31: Warning: \n",
      " * Caution : The Input Model is CHANGED because inplace=True.\n",
      "  warnings.warn(\"\\n * Caution : The Input Model is CHANGED because inplace=True.\", Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Linear -> BayesLinear\n",
    "transform_model(model, nn.Linear, bnn.BayesLinear, \n",
    "            args={\"prior_mu\":0, \"prior_sigma\":0.1, \"in_features\" : \".in_features\",\n",
    "                  \"out_features\" : \".out_features\", \"bias\":\".bias\"\n",
    "                 }, \n",
    "            attrs={\"weight_mu\" : \".weight\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91832ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0cf4f14a0383>:31: Warning: \n",
      " * Caution : The Input Model is CHANGED because inplace=True.\n",
      "  warnings.warn(\"\\n * Caution : The Input Model is CHANGED because inplace=True.\", Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): BayesConv2d(0, 0.1, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BayesConv2d(0, 0.1, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_model(model, nn.Conv2d, bnn.BayesConv2d, \n",
    "                args={\"prior_mu\":0, \"prior_sigma\":0.1, \"in_channels\" : \".in_channels\",\n",
    "                      \"out_channels\" : \".out_channels\", \"kernel_size\" : \".kernel_size\",\n",
    "                      \"stride\" : \".stride\", \"padding\" : \".padding\", \"bias\":\".bias\"\n",
    "                     }, \n",
    "                attrs={\"weight_mu\" : \".weight\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ab27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "#model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8d290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-0cf4f14a0383>:31: Warning: \n",
      " * Caution : The Input Model is CHANGED because inplace=True.\n",
      "  warnings.warn(\"\\n * Caution : The Input Model is CHANGED because inplace=True.\", Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): BayesConv2d(0, 0.1, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BayesConv2d(0, 0.1, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_model(model, nn.Conv2d, bnn.BayesConv2d, \n",
    "                args={\"prior_mu\":0, \"prior_sigma\":0.1, \"in_channels\" : \".in_channels\",\n",
    "                      \"out_channels\" : \".out_channels\", \"kernel_size\" : \".kernel_size\",\n",
    "                      \"stride\" : \".stride\", \"padding\" : \".padding\", \"bias\":\".bias\"\n",
    "                     }, \n",
    "                attrs={\"weight_mu\" : \".weight\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2631147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2127304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unocmmment for BNN model\n",
    "net = nn.Sequential(\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.01, in_features=150528, out_features=1505),\n",
    "    nn.ReLU(),\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.01, in_features=1505, out_features=1505),\n",
    ")\n",
    "net\n",
    "model=net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510fbfe",
   "metadata": {},
   "source": [
    "#Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.8 )\n",
    "#bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.01, base_momentum = 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ccbb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "pred_total =[]\n",
    "target_total = []\n",
    "f1_train_total = []\n",
    "f1_val_total = []\n",
    "total_step = len(train_dataloader)\n",
    "#since = time.time()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_dataloader):\n",
    "\n",
    "        data_ = data_.view(data_.size(0), -1) #-Convert to be readable by model(Used only in BNN, Linear)\n",
    " \n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "            \n",
    "        # Compute prediction and loss\n",
    "        outputs = net(data_.float())\n",
    "       \n",
    "        #ce_loss =criterion(outputs, target_.flatten().type(torch.LongTensor))\n",
    "        #kl = kl_loss(net)\n",
    "        #loss = ce_loss+kl\n",
    "        loss =criterion(outputs, target_.flatten().type(torch.LongTensor))\n",
    "        \n",
    "        \n",
    "        # Backpropagation and param tunning\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        #STATS\n",
    "        running_loss += loss.item()       \n",
    "        _,pred = torch.max(outputs.float(), dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        #lr=config[\"lr\"]\n",
    "        #print(pred)\n",
    "        #print(target_)\n",
    "        f1 = f1_score(target_, pred, average='micro')\n",
    "        #train_acc.append(100 * correct / total)\n",
    "        f1_train_total.append(f1)\n",
    "        #print(f1)\n",
    "        pred_total.append(pred)\n",
    "        target_total.append(target_)\n",
    "        batch_size =int(config[\"batch_size\"])\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], LR:[{}], batch_size:[{}], Loss: {:.4f}, f1: {:.4f},acc: {:.4f}'\n",
    "                   .format(epoch, n_epochs, batch_idx, total_step,optimizer.param_groups[0]['lr'],batch_size, loss.item(),f1,(100 * correct/total)))\n",
    "           # dateTimeObj = datetime.now()\n",
    "           # print(dateTimeObj)\n",
    "           #print(optimizer.param_groups[0]['lr'])\n",
    "           # print(lr_scheduler.get_lr())\n",
    "    #torch.save(pred_total, 'pred_total.pt')\n",
    "    #torch.save(target_total, 'target_total.pt')\n",
    "    scheduler.step()#to test\n",
    "\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, f1: {(f1):.4f}')\n",
    "    print(f'\\correct: {correct:.4f}, total: {total:.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net.eval() # switch to eval mode\n",
    "        for data_t, target_t in (val_dataloader):\n",
    "            data_ = data_.view(data_.size(0), -1) #-Convert to be readable by model(Used only in BNN, Linear)\n",
    "            \n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net(data_t.float())\n",
    "            #loss_t = criterion(outputs_t.float(), target_t.flatten().type(torch.LongTensor))\n",
    "            #ce_loss_t =criterion(outputs_t.float(), target_t.flatten().type(torch.LongTensor))\n",
    "            #kl_t = kl_loss(net)\n",
    "            #loss_t = ce_loss+kl_t\n",
    "            batch_loss += loss_t.item()\n",
    "            \n",
    "            _,pred_t = torch.max(outputs_t.float(), dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            #print(pred_t)\n",
    "            #print(target_t)\n",
    "            total_t += target_t.size(0)\n",
    "            f1_val = f1_score(target_t, pred_t, average='micro')\n",
    "            f1_val_total.append(f1_val)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(val_dataloader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation f1: {(f1_val):}\\n') #{(100 * correct_t/total_t):.4f}\\n')\n",
    "        print(f'corret: {correct_t:.4f}, total: {total_t:.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "        #save results\n",
    "        f1_val_total.append(f1_val)\n",
    "        #Pickling results\n",
    "        #Pickling results\n",
    "        with open(\"bnn_train_acc.txt\", \"wb\") as fp:   \n",
    "                pickle.dump(train_acc, fp)\n",
    "        with open(\"bnn_train_loss.txt\", \"wb\") as fp:   \n",
    "                pickle.dump(train_loss, fp)\n",
    "        with open(\"bnn_val_acc.txt\", \"wb\") as fp:   \n",
    "                pickle.dump(val_acc, fp)\n",
    "        with open(\"bnn_val_loss.txt\", \"wb\") as fp:   \n",
    "                pickle.dump(val_loss, fp)\n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            print('Improvement-Detected, save-model')\n",
    "            torch.save(net.state_dict(), 'BNN.pt')\n",
    "        \n",
    "\n",
    "    net.train()# switch back to train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3587c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bnn_train_acc.txt\", \"rb\") as fp:   \n",
    "        train_acc = pickle.load(fp)\n",
    "with open(\"bnn_val_acc.txt\", \"rb\") as fp:   \n",
    "        val_acc = pickle.load(fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ede7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('testlist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a7137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test, int(config[\"batch_size\"]),\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f68ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"lr\": np.random.uniform(1e-4),\n",
    "    \"batch_size\": np.random.choice([64]),\n",
    "    \"momentum\": np.random.uniform( 0.8)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec8a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net = ConvNet()\n",
    "net.load_state_dict(torch.load('BNN.pt'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281a3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, config[\"lr\"], max_lr=config[\"lr\"],  base_momentum = config[\"momentum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce92a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_min = np.Inf\n",
    "batch_loss = 0\n",
    "total_t=0\n",
    "correct_t=0\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "pred_total =[]\n",
    "target_total = []\n",
    "torchpred_total = torch.tensor([])\n",
    "torchtarget_total = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for data_t, target_t in (test_dataloader):\n",
    "        #Convert to be readable by model\n",
    "        #data_t = data_t.view(data_t.size(0), -1)\n",
    "            \n",
    "        data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "        outputs_t = net(data_t.float())\n",
    "        #loss_t = criterion(outputs_t.float(), target_t.flatten().type(torch.LongTensor))\n",
    "        ce_loss_t =criterion(outputs_t, target_t.flatten().type(torch.LongTensor))\n",
    "        kl_t = kl_loss(net)\n",
    "        loss_t = ce_loss_t+kl_t\n",
    "       # loss =criterion(outputs, target_.flatten().type(torch.LongTensor))\n",
    "        \n",
    "        batch_loss += loss_t.item()\n",
    "        _,pred_t = torch.max(outputs_t.float(), dim=1)\n",
    "        correct_t += torch.sum(pred_t==target_t).item()\n",
    "        total_t += target_t.size(0)\n",
    "        print(pred_t)\n",
    "        print(target_t)\n",
    "        pred_total.append(pred_t)\n",
    "        target_total.append(target_t)\n",
    "\n",
    "    val_acc.append(100 * correct_t/total_t)\n",
    "    val_loss.append(batch_loss/len(test_dataloader))\n",
    "    network_learned = batch_loss < valid_loss_min\n",
    "    print(f'test loss: {np.mean(val_loss):.4f}, test acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "    print(f'corret: {correct_t:.4f}, total: {total_t:.4f}\\n')\n",
    "torch.save(pred_total, 'BNN_pred_total.pt')\n",
    "torch.save(target_total, 'BNN_target_total.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
